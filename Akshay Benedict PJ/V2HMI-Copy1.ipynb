{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d36067-b24f-4724-ae28-3c322b5282f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote app_streamlit.py — next: run it with Streamlit.\n"
     ]
    }
   ],
   "source": [
    "app_code = r'''\n",
    "import streamlit as st\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# ---------- compatibility for older/newer streamlit caching ----------\n",
    "try:\n",
    "    cache_resource = st.cache_resource\n",
    "    cache_data = st.cache_data\n",
    "except Exception:\n",
    "    # fallback for older Streamlit\n",
    "    def cache_resource(func):\n",
    "        return st.cache(allow_output_mutation=True)(func)\n",
    "    def cache_data(func):\n",
    "        return st.cache(func)\n",
    "\n",
    "# ---------- Data / model loaders ----------\n",
    "@cache_resource\n",
    "def load_index_and_model(chunks_path):\n",
    "    # chunks_path: path to jsonl where each line is {\"text\": \"...\", \"embedding\": [...], \"source\": \"...\"}\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    chunks = []\n",
    "    with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            chunks.append(json.loads(line))\n",
    "    embeddings = np.array([c[\"embedding\"] for c in chunks], dtype=\"float32\")\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(embeddings)\n",
    "    return model, index, chunks\n",
    "\n",
    "@cache_resource\n",
    "def load_generator(device=-1):\n",
    "    # device = -1 for CPU, 0+ for GPU index\n",
    "    return pipeline(\"text2text-generation\",\n",
    "                    model=\"google/flan-t5-base\",\n",
    "                    tokenizer=\"google/flan-t5-base\",\n",
    "                    device=device)\n",
    "\n",
    "# ---------- Retrieval + generation (adapted from your notebook) ----------\n",
    "\n",
    "# ---------- Retrieval + generation (adapted from your notebook) ----------\n",
    "def retrieve_chunks(query, model, index, chunks, top_k=5, min_score=0.3):\n",
    "    \"\"\"\n",
    "    Retrieve top chunks for a query from FAISS index.\n",
    "    Only return chunks with similarity >= min_score.\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    q_emb = model.encode(query, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb.reshape(1, -1)) \n",
    "    # Search\n",
    "    D, I = index.search(q_emb.reshape(1, -1), top_k)  \n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if score >= min_score:\n",
    "            results.append(chunks[idx])\n",
    "    return results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Answer generation\n",
    "# -----------------------------\n",
    "def generate_answer(query, model, index, chunks, top_k=5, generator=None, min_score=0.3, max_new_tokens=200):\n",
    "    retrieved = retrieve_chunks(query, model, index, chunks, top_k=top_k, min_score=min_score)\n",
    "    if not retrieved:\n",
    "        return \"No relevant information found.\", []\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"Source: {r.get('source', 'Unknown')}\\n{r['text']}\" for r in retrieved])\n",
    "    citations = [f\"{r.get('source', 'Unknown')} — {r['text']}\" for r in retrieved]\n",
    "\n",
    "    if generator is not None:\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert eligibility officer.\n",
    "        Using only the context below, answer the question truthfully.\n",
    "        If the answer is not in the context, say \"I cannot find relevant information.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        output = generator(prompt, max_new_tokens=max_new_tokens)\n",
    "        return output[0].get(\"generated_text\", output[0].get(\"text\", \"\")).strip(), citations\n",
    "\n",
    "    answer = f\"Here is what I found based on the documents:\\n\\n{context}\"\n",
    "    return answer, citations\n",
    "\n",
    "\n",
    "# ---------- Streamlit UI ----------\n",
    "st.set_page_config(page_title=\"RAG Visa-eligibility HMI\", layout=\"wide\")\n",
    "st.title(\"RAG Visa-Eligibility — Streamlit HMI (Demo)\")\n",
    "\n",
    "# Sidebar config\n",
    "st.sidebar.header(\"Settings\")\n",
    "uploaded = st.sidebar.file_uploader(\"Upload chunks (.jsonl) — each line: {text, embedding, source}\", type=[\"jsonl\"])\n",
    "default_path = st.sidebar.text_input(\"Chunks JSONL path (if no upload)\", value=r\"E:\\\\Info_Srping\\\\swiftvisa\\\\index\\\\chunks_with_embeddings_v2.jsonl\")\n",
    "use_generator = st.sidebar.checkbox(\"Use Generator (LLM) for final answer\", value=True)\n",
    "device_option = st.sidebar.selectbox(\"Generator device\", options=[\"cpu\", \"gpu\"], index=0)\n",
    "top_k = st.sidebar.slider(\"Top K retrieved chunks\", 1, 10, 5)\n",
    "max_new_tokens = st.sidebar.slider(\"Generator: max_new_tokens\", 50, 1000, 200)\n",
    "\n",
    "# load resources (uploaded file overrides the path)\n",
    "chunks_path = None\n",
    "if uploaded is not None:\n",
    "    # save temporarily\n",
    "    tmp = \"uploaded_chunks.jsonl\"\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        f.write(uploaded.getbuffer())\n",
    "    chunks_path = tmp\n",
    "else:\n",
    "    chunks_path = default_path\n",
    "\n",
    "st.sidebar.markdown(\"**Index path:**\")\n",
    "st.sidebar.code(chunks_path)\n",
    "\n",
    "# Load models / index (cached)\n",
    "status = st.empty()\n",
    "with status.container():\n",
    "    st.write(\"Loading embedding model + FAISS index (cached) ...\")\n",
    "try:\n",
    "    model, index, chunks = load_index_and_model(chunks_path)\n",
    "    st.success(f\"Loaded index with {len(chunks)} chunks (embedding dim = {index.d})\")\n",
    "except Exception as e:\n",
    "    st.error(f\"Failed to load index/model: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "generator = None\n",
    "if use_generator:\n",
    "    dev = -1 if device_option == \"cpu\" else 0\n",
    "    try:\n",
    "        with st.spinner(\"Loading generator...\"):\n",
    "            generator = load_generator(device=dev)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load generator: {e}\")\n",
    "        generator = None\n",
    "\n",
    "# Query input\n",
    "st.markdown(\"### Ask a question about visa eligibility\")\n",
    "query = st.text_area(\"Enter your question\", value=\"What are the eligibility requirements for a UK Student Visa?\", height=120)\n",
    "ask = st.button(\"Get Answer\")\n",
    "\n",
    "if ask and query.strip():\n",
    "    with st.spinner(\"Retrieving top chunks ...\"):\n",
    "        retrieved = retrieve_chunks(query, model, index, chunks, top_k=top_k)\n",
    "    # show retrieved pieces\n",
    "    st.markdown(\"#### Retrieved context (top results)\")\n",
    "    for i, r in enumerate(retrieved, start=1):\n",
    "        src = r.get(\"source\", \"Unknown\")\n",
    "        st.markdown(f\"**{i}. Source:** {src}\")\n",
    "        st.write(r.get(\"text\", \"[no text]\"))\n",
    "\n",
    "    # generate final answer\n",
    "    with st.spinner(\"Generating answer ...\"):\n",
    "        answer, citations = generate_answer(query, model, index, chunks, top_k=top_k, generator=generator, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    st.markdown(\"### Final Answer\")\n",
    "    st.write(answer)\n",
    "\n",
    "    st.markdown(\"### Citations / matched chunks\")\n",
    "    for c in citations:\n",
    "        st.write(\"- \", c[:1000])  # truncate very long chunks in UI\n",
    "\n",
    "    st.download_button(\"Download answer + citations (txt)\", data=answer + \"\\\\n\\\\nCITATIONS:\\\\n\" + \"\\\\n\".join(citations),\n",
    "                       file_name=\"answer_and_citations.txt\")\n",
    "else:\n",
    "    st.info(\"Enter a question and click **Get Answer**. You can upload a chunks .jsonl or provide a local path in the sidebar.\")\n",
    "'''\n",
    "# write to file\n",
    "with open(\"app_streamlit.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"Wrote app_streamlit.py — next: run it with Streamlit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9077771-3fe2-4920-8707-9b2f651fbf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit started (PID 9492). Open http://localhost:8501\n"
     ]
    }
   ],
   "source": [
    "# spawn streamlit in background from notebook (works locally)\n",
    "import subprocess, sys, time, webbrowser\n",
    "proc = subprocess.Popen([sys.executable, \"-m\", \"streamlit\", \"run\", \"app_streamlit.py\"],\n",
    "                        stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"Streamlit started (PID {}). Open http://localhost:8501\".format(proc.pid))\n",
    "# optional: try to open automatically\n",
    "try:\n",
    "    webbrowser.open(\"http://localhost:8501\")\n",
    "except:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
