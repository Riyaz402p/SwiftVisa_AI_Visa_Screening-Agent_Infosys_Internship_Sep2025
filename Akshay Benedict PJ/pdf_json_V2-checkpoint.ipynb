{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32072304-cb69-4b65-ba2c-bda894d22561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_slug(s: str, repl=\"-\"):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    # Replace any character not alnum with hyphen\n",
    "    s = re.sub(r\"[^a-z0-9]+\", repl, s)\n",
    "    # Collapse multiple hyphens\n",
    "    s = re.sub(rf\"{repl}{{2,}}\", repl, s)\n",
    "    # Trim leading/trailing hyphens\n",
    "    s = s.strip(repl)\n",
    "    return s or \"na\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fd6b5d-a7aa-4d1d-82da-eb1d4303d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# MiniLM compatible tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def chunk_by_tokens(text: str, max_tokens=256, overlap=32):\n",
    "    ids = tok.encode(text, add_special_tokens=False)\n",
    "    n = len(ids)\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end = min(start + max_tokens, n)\n",
    "        chunk_ids = ids[start:end]\n",
    "        chunk_text = tok.decode(chunk_ids, skip_special_tokens=True)\n",
    "        # Find the character start/end by re-encoding chunk_text if needed\n",
    "        yield start, end, chunk_text\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(end - overlap, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "806414b4-2245-4969-a4d9-aee503ab8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_chunks_tokenwise(pdf_path, meta, out_path, max_tokens=256, overlap=32):\n",
    "    import fitz, time, json, hashlib\n",
    "    from pathlib import Path\n",
    "\n",
    "    pdf_path = Path(pdf_path)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "\n",
    "    # SHA256\n",
    "    h = hashlib.sha256()\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        for b in iter(lambda: f.read(1 << 20), b\"\"):\n",
    "            h.update(b)\n",
    "    docsha = h.hexdigest()\n",
    "    meta[\"docsha256\"] = docsha\n",
    "\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    all_chunks = []\n",
    "    seq = 1\n",
    "\n",
    "    country_slug = normalize_slug(meta[\"country\"])\n",
    "    visa_slug = normalize_slug(meta[\"visa_type\"])\n",
    "    year_slug = normalize_slug(str(meta[\"year\"]))\n",
    "    doc_slug = normalize_slug(meta[\"doc_slug\"])\n",
    "\n",
    "    for pagenum in range(len(doc)):\n",
    "        page = doc[pagenum]\n",
    "        text = page.get_text(\"text\")\n",
    "        # Normalize whitespace/control chars to spaces first\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        for tstart, tend, ctext in chunk_by_tokens(text, max_tokens, overlap):\n",
    "            docid = f\"{country_slug}-{visa_slug}-{year_slug}-{doc_slug}\"\n",
    "            chunkid = f\"{docid}-Pg{pagenum+1}-seq{seq:03d}\"\n",
    "            chunkmeta = {\n",
    "                \"chunkid\": chunkid,\n",
    "                \"docid\": docid,\n",
    "                \"source\": meta.get(\"source\"),\n",
    "                \"url\": meta.get(\"url\"),\n",
    "                \"country\": meta.get(\"country\"),\n",
    "                \"visa_type\": meta.get(\"visatype\"),\n",
    "                \"effectivedate\": meta.get(\"effectivedate\"),\n",
    "                \"version\": meta.get(\"version\"),\n",
    "                \"docsha256\": docsha,\n",
    "                \"retrievedat\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "                \"page\": pagenum + 1,\n",
    "                \"pages\": pagenum + 1,\n",
    "                \"sectiontitle\": None,\n",
    "                \"language\": \"en\",\n",
    "                \"token_start\": int(tstart),\n",
    "                \"token_end\": int(tend),\n",
    "                \"text\": ctext,\n",
    "            }\n",
    "            all_chunks.append(chunkmeta)\n",
    "            seq += 1\n",
    "\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ch in all_chunks:\n",
    "            f.write(json.dumps(ch, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved {len(all_chunks)} chunks to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d097df10-f3db-41d3-b10c-13a9dca7a05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2899 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 253 chunks to E:\\Info_Srping\\swiftvisa\\data\\processed\\UK_StudentVisa_chunks_new.jsonl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_file = r\"E:\\Info_Srping\\swiftvisa\\data\\raw\\UK_Student_Visa.pdf\"\n",
    "    output_file = r\"E:\\Info_Srping\\swiftvisa\\data\\processed\\UK_StudentVisa_chunks_new.jsonl\"\n",
    "\n",
    "    meta_info = {\n",
    "        \"country\": \"UK\",\n",
    "        \"visa_type\": \"Student and Child Student\",\n",
    "        \"year\": \"2025\",\n",
    "        \"doc_slug\": \"UK Student Visa Guide 2024\",\n",
    "        \"source\": \"Student and Child Student\",\n",
    "        \"url\": \"https://gov.uk/student-visa\",\n",
    "        \"effective_date\": \"2025-07-16\",\n",
    "        \"version\": \"11.0\"\n",
    "    }\n",
    "\n",
    "    pdf_to_chunks_tokenwise(pdf_file, meta_info, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
