{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "750acb71-e05d-4fcd-8309-fb9eabe23253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote app_streamlit.py — next: run it with Streamlit.\n"
     ]
    }
   ],
   "source": [
    "app_code = r'''\n",
    "import streamlit as st\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# ---------- compatibility for older/newer streamlit caching ----------\n",
    "try:\n",
    "    cache_resource = st.cache_resource\n",
    "    cache_data = st.cache_data\n",
    "except Exception:\n",
    "    # fallback for older Streamlit\n",
    "    def cache_resource(func):\n",
    "        return st.cache(allow_output_mutation=True)(func)\n",
    "    def cache_data(func):\n",
    "        return st.cache(func)\n",
    "\n",
    "# ---------- Data / model loaders ----------\n",
    "@cache_resource\n",
    "def load_index_and_model(chunks_path):\n",
    "    # chunks_path: path to jsonl where each line is {\"text\": \"...\", \"embedding\": [...], \"source\": \"...\"}\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    chunks = []\n",
    "    with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            chunks.append(json.loads(line))\n",
    "    embeddings = np.array([c[\"embedding\"] for c in chunks], dtype=\"float32\")\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(embeddings)\n",
    "    return model, index, chunks\n",
    "\n",
    "@cache_resource\n",
    "def load_generator(device=-1):\n",
    "    # device = -1 for CPU, 0+ for GPU index\n",
    "    return pipeline(\"text2text-generation\",\n",
    "                    model=\"google/flan-t5-base\",\n",
    "                    tokenizer=\"google/flan-t5-base\",\n",
    "                    device=device)\n",
    "\n",
    "# ---------- Retrieval + generation (adapted from your notebook) ----------\n",
    "\n",
    "# ---------- Retrieval + generation (adapted from your notebook) ----------\n",
    "def retrieve_chunks(query, model, index, chunks, top_k=5, min_score=0.3):\n",
    "    \"\"\"\n",
    "    Retrieve top chunks for a query from FAISS index.\n",
    "    Only return chunks with similarity >= min_score.\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    q_emb = model.encode(query, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb.reshape(1, -1)) \n",
    "    # Search\n",
    "    D, I = index.search(q_emb.reshape(1, -1), top_k)  \n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if score >= min_score:\n",
    "            results.append(chunks[idx])\n",
    "    return results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Answer generation\n",
    "# -----------------------------\n",
    "def generate_answer(query, model, index, chunks, top_k=5, generator=None, min_score=0.3, max_new_tokens=200):\n",
    "    retrieved = retrieve_chunks(query, model, index, chunks, top_k=top_k, min_score=min_score)\n",
    "    if not retrieved:\n",
    "        return \"No relevant information found.\", []\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"Source: {r.get('source', 'Unknown')}\\n{r['text']}\" for r in retrieved])\n",
    "    citations = [f\"{r.get('source', 'Unknown')} — {r['text']}\" for r in retrieved]\n",
    "\n",
    "    if generator is not None:\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert eligibility officer.\n",
    "        Using only the context below, answer the question truthfully.\n",
    "        If the answer is not in the context, say \"I cannot find relevant information.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        output = generator(prompt, max_new_tokens=max_new_tokens)\n",
    "        return output[0].get(\"generated_text\", output[0].get(\"text\", \"\")).strip(), citations\n",
    "\n",
    "    answer = f\"Here is what I found based on the documents:\\n\\n{context}\"\n",
    "    return answer, citations\n",
    "\n",
    "\n",
    "# ---------- Streamlit UI ----------\n",
    "st.set_page_config(page_title=\"RAG Visa-eligibility HMI\", layout=\"wide\")\n",
    "st.title(\"RAG Visa-Eligibility — Streamlit HMI (Demo)\")\n",
    "\n",
    "# Sidebar config\n",
    "st.sidebar.header(\"Settings\")\n",
    "uploaded = st.sidebar.file_uploader(\"Upload chunks (.jsonl) — each line: {text, embedding, source}\", type=[\"jsonl\"])\n",
    "default_path = st.sidebar.text_input(\"Chunks JSONL path (if no upload)\", value=r\"C:\\Users\\ASUS\\OneDrive\\Desktop\\chunks_with_embeddings_v2.jsonl\")\n",
    "use_generator = st.sidebar.checkbox(\"Use Generator (LLM) for final answer\", value=True)\n",
    "device_option = st.sidebar.selectbox(\"Generator device\", options=[\"cpu\", \"gpu\"], index=0)\n",
    "top_k = st.sidebar.slider(\"Top K retrieved chunks\", 1, 10, 5)\n",
    "max_new_tokens = st.sidebar.slider(\"Generator: max_new_tokens\", 50, 1000, 200)\n",
    "\n",
    "# load resources (uploaded file overrides the path)\n",
    "chunks_path = None\n",
    "if uploaded is not None:\n",
    "    # save temporarily\n",
    "    tmp = \"uploaded_chunks.jsonl\"\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        f.write(uploaded.getbuffer())\n",
    "    chunks_path = tmp\n",
    "else:\n",
    "    chunks_path = default_path\n",
    "\n",
    "st.sidebar.markdown(\"**Index path:**\")\n",
    "st.sidebar.code(chunks_path)\n",
    "\n",
    "# Load models / index (cached)\n",
    "status = st.empty()\n",
    "with status.container():\n",
    "    st.write(\"Loading embedding model + FAISS index (cached) ...\")\n",
    "try:\n",
    "    model, index, chunks = load_index_and_model(chunks_path)\n",
    "    st.success(f\"Loaded index with {len(chunks)} chunks (embedding dim = {index.d})\")\n",
    "except Exception as e:\n",
    "    st.error(f\"Failed to load index/model: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "generator = None\n",
    "if use_generator:\n",
    "    dev = -1 if device_option == \"cpu\" else 0\n",
    "    try:\n",
    "        with st.spinner(\"Loading generator...\"):\n",
    "            generator = load_generator(device=dev)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load generator: {e}\")\n",
    "        generator = None\n",
    "\n",
    "# Query input\n",
    "st.markdown(\"### Ask a question about visa eligibility\")\n",
    "query = st.text_area(\"Enter your question\", value=\"What are the eligibility requirements for a UK Student Visa?\", height=120)\n",
    "ask = st.button(\"Get Answer\")\n",
    "\n",
    "if ask and query.strip():\n",
    "    with st.spinner(\"Retrieving top chunks ...\"):\n",
    "        retrieved = retrieve_chunks(query, model, index, chunks, top_k=top_k)\n",
    "    # show retrieved pieces\n",
    "    st.markdown(\"#### Retrieved context (top results)\")\n",
    "    for i, r in enumerate(retrieved, start=1):\n",
    "        src = r.get(\"source\", \"Unknown\")\n",
    "        st.markdown(f\"**{i}. Source:** {src}\")\n",
    "        st.write(r.get(\"text\", \"[no text]\"))\n",
    "\n",
    "    # generate final answer\n",
    "    with st.spinner(\"Generating answer ...\"):\n",
    "        answer, citations = generate_answer(query, model, index, chunks, top_k=top_k, generator=generator, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    st.markdown(\"### Final Answer\")\n",
    "    st.write(answer)\n",
    "\n",
    "    st.markdown(\"### Citations / matched chunks\")\n",
    "    for c in citations:\n",
    "        st.write(\"- \", c[:1000])  # truncate very long chunks in UI\n",
    "\n",
    "    st.download_button(\"Download answer + citations (txt)\", data=answer + \"\\\\n\\\\nCITATIONS:\\\\n\" + \"\\\\n\".join(citations),\n",
    "                       file_name=\"answer_and_citations.txt\")\n",
    "else:\n",
    "    st.info(\"Enter a question and click **Get Answer**. You can upload a chunks .jsonl or provide a local path in the sidebar.\")\n",
    "'''\n",
    "# write to file\n",
    "with open(\"app_streamlit.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"Wrote app_streamlit.py — next: run it with Streamlit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de6186f6-ad28-4849-888f-62e84390cdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp313-cp313-win_amd64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\asus\\appdata\\roaming\\python\\python313\\site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\anaconda\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.12.0-cp313-cp313-win_amd64.whl (18.2 MB)\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/18.2 MB 428.1 kB/s eta 0:00:42\n",
      "   - -------------------------------------- 0.5/18.2 MB 428.1 kB/s eta 0:00:42\n",
      "   -- ------------------------------------- 1.0/18.2 MB 711.8 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 1.3/18.2 MB 828.6 kB/s eta 0:00:21\n",
      "   -- ------------------------------------- 1.3/18.2 MB 828.6 kB/s eta 0:00:21\n",
      "   -- ------------------------------------- 1.3/18.2 MB 828.6 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.6/18.2 MB 719.8 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.6/18.2 MB 719.8 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.8/18.2 MB 677.9 kB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 2.1/18.2 MB 742.2 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 2.6/18.2 MB 867.4 kB/s eta 0:00:18\n",
      "   ------ --------------------------------- 3.1/18.2 MB 965.2 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 3.4/18.2 MB 1.0 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 3.4/18.2 MB 1.0 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 3.4/18.2 MB 1.0 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 3.7/18.2 MB 932.5 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 3.9/18.2 MB 921.6 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 4.5/18.2 MB 1.0 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 5.0/18.2 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 5.5/18.2 MB 1.1 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 6.0/18.2 MB 1.2 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 6.3/18.2 MB 1.2 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 6.6/18.2 MB 1.2 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 6.8/18.2 MB 1.2 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 6.8/18.2 MB 1.2 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 7.1/18.2 MB 1.2 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 7.1/18.2 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 8.1/18.2 MB 1.3 MB/s eta 0:00:09\n",
      "   ------------------- -------------------- 8.9/18.2 MB 1.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 9.4/18.2 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 10.2/18.2 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 11.3/18.2 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 11.8/18.2 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 12.8/18.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 13.6/18.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 14.4/18.2 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 15.2/18.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 16.0/18.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 16.8/18.2 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.3/18.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.8/18.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.1/18.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.2/18.2 MB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d853cfc4-5ac7-49b4-a206-28cac9fa1631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:24:12.994 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:12.996 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.167 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\ASUS\\anaconda\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-08 00:24:13.168 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.169 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.169 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.172 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.178 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.180 Session state does not function when running a script without `streamlit run`\n",
      "2025-10-08 00:24:13.181 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.182 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.183 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.186 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.187 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.189 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.190 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.192 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.193 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.194 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.195 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.196 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.198 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.199 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.201 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.202 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.206 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.208 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.209 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.210 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.212 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.213 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.214 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.216 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.217 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.218 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.219 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.221 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.224 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.226 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.228 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.230 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.740 Thread 'Thread-24': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:13.743 Thread 'Thread-24': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:18.644 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:18.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:18.662 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:18.662 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:18.664 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:18.665 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:18.666 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:18.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:18.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:18.672 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:19.176 Thread 'Thread-26': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:19.178 Thread 'Thread-25': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:19.179 Thread 'Thread-26': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:24:19.182 Thread 'Thread-25': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3060e02b4baa47cdb3316424560e8a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fc122b0e924ee099fbc65d9efbc363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f61c6f7ef074c28bba165d416ec22bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6783ac55ab514c6aa17401ee47b3af76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf2214bf9904b70a9a73e2b75f9af50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb6100e88604b56910c30cf1ae937b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb321c895514ea6a02d3e084a4b8788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "2025-10-08 00:31:25.582 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.583 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.584 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.585 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.586 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.588 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.589 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.590 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.591 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.592 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.593 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.594 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.595 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.596 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.597 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.598 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.599 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.600 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-08 00:31:25.601 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote app_streamlit.py — next: run it with Streamlit.\n"
     ]
    }
   ],
   "source": [
    "# app_code = r'''\n",
    "import streamlit as st\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# ---------- compatibility for older/newer streamlit caching ----------\n",
    "try:\n",
    "    cache_resource = st.cache_resource\n",
    "    cache_data = st.cache_data\n",
    "except Exception:\n",
    "    # fallback for older Streamlit\n",
    "    def cache_resource(func):\n",
    "        return st.cache(allow_output_mutation=True)(func)\n",
    "    def cache_data(func):\n",
    "        return st.cache(func)\n",
    "\n",
    "# ---------- Data / model loaders ----------\n",
    "@cache_resource\n",
    "def load_index_and_model(chunks_path):\n",
    "    # chunks_path: path to jsonl where each line is {\"text\": \"...\", \"embedding\": [...], \"source\": \"...\"}\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    chunks = []\n",
    "    with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            chunks.append(json.loads(line))\n",
    "    embeddings = np.array([c[\"embedding\"] for c in chunks], dtype=\"float32\")\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(embeddings)\n",
    "    return model, index, chunks\n",
    "\n",
    "@cache_resource\n",
    "def load_generator(device=-1):\n",
    "    # device = -1 for CPU, 0+ for GPU index\n",
    "    return pipeline(\"text2text-generation\",\n",
    "                    model=\"google/flan-t5-base\",\n",
    "                    tokenizer=\"google/flan-t5-base\",\n",
    "                    device=device)\n",
    "\n",
    "# ---------- Retrieval + generation (adapted from your notebook) ----------\n",
    "\n",
    "# ---------- Retrieval + generation (adapted from your notebook) ----------\n",
    "def retrieve_chunks(query, model, index, chunks, top_k=5, min_score=0.3):\n",
    "    \"\"\"\n",
    "    Retrieve top chunks for a query from FAISS index.\n",
    "    Only return chunks with similarity >= min_score.\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    q_emb = model.encode(query, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb.reshape(1, -1)) \n",
    "    # Search\n",
    "    D, I = index.search(q_emb.reshape(1, -1), top_k)  \n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if score >= min_score:\n",
    "            results.append(chunks[idx])\n",
    "    return results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Answer generation\n",
    "# -----------------------------\n",
    "def generate_answer(query, model, index, chunks, top_k=5, generator=None, min_score=0.3, max_new_tokens=200):\n",
    "    retrieved = retrieve_chunks(query, model, index, chunks, top_k=top_k, min_score=min_score)\n",
    "    if not retrieved:\n",
    "        return \"No relevant information found.\", []\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"Source: {r.get('source', 'Unknown')}\\n{r['text']}\" for r in retrieved])\n",
    "    citations = [f\"{r.get('source', 'Unknown')} — {r['text']}\" for r in retrieved]\n",
    "\n",
    "    if generator is not None:\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert eligibility officer.\n",
    "        Using only the context below, answer the question truthfully.\n",
    "        If the answer is not in the context, say \"I cannot find relevant information.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        output = generator(prompt, max_new_tokens=max_new_tokens)\n",
    "        return output[0].get(\"generated_text\", output[0].get(\"text\", \"\")).strip(), citations\n",
    "\n",
    "    answer = f\"Here is what I found based on the documents:\\n\\n{context}\"\n",
    "    return answer, citations\n",
    "\n",
    "\n",
    "# ---------- Streamlit UI ----------\n",
    "st.set_page_config(page_title=\"RAG Visa-eligibility HMI\", layout=\"wide\")\n",
    "st.title(\"RAG Visa-Eligibility — Streamlit HMI (Demo)\")\n",
    "\n",
    "# Sidebar config\n",
    "st.sidebar.header(\"Settings\")\n",
    "uploaded = st.sidebar.file_uploader(\"Upload chunks (.jsonl) — each line: {text, embedding, source}\", type=[\"jsonl\"])\n",
    "default_path = st.sidebar.text_input(\"Chunks JSONL path (if no upload)\", value=r\"C:\\Users\\ASUS\\OneDrive\\Desktop\\chunks_with_embeddings_v2.jsonl\")\n",
    "use_generator = st.sidebar.checkbox(\"Use Generator (LLM) for final answer\", value=True)\n",
    "device_option = st.sidebar.selectbox(\"Generator device\", options=[\"cpu\", \"gpu\"], index=0)\n",
    "top_k = st.sidebar.slider(\"Top K retrieved chunks\", 1, 10, 5)\n",
    "max_new_tokens = st.sidebar.slider(\"Generator: max_new_tokens\", 50, 1000, 200)\n",
    "\n",
    "# load resources (uploaded file overrides the path)\n",
    "chunks_path = None\n",
    "if uploaded is not None:\n",
    "    # save temporarily\n",
    "    tmp = \"uploaded_chunks.jsonl\"\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        f.write(uploaded.getbuffer())\n",
    "    chunks_path = tmp\n",
    "else:\n",
    "    chunks_path = default_path\n",
    "\n",
    "st.sidebar.markdown(\"**Index path:**\")\n",
    "st.sidebar.code(chunks_path)\n",
    "\n",
    "# Load models / index (cached)\n",
    "status = st.empty()\n",
    "with status.container():\n",
    "    st.write(\"Loading embedding model + FAISS index (cached) ...\")\n",
    "try:\n",
    "    model, index, chunks = load_index_and_model(chunks_path)\n",
    "    st.success(f\"Loaded index with {len(chunks)} chunks (embedding dim = {index.d})\")\n",
    "except Exception as e:\n",
    "    st.error(f\"Failed to load index/model: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "generator = None\n",
    "if use_generator:\n",
    "    dev = -1 if device_option == \"cpu\" else 0\n",
    "    try:\n",
    "        with st.spinner(\"Loading generator...\"):\n",
    "            generator = load_generator(device=dev)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load generator: {e}\")\n",
    "        generator = None\n",
    "\n",
    "# Query input\n",
    "st.markdown(\"### Ask a question about visa eligibility\")\n",
    "query = st.text_area(\"Enter your question\", value=\"What are the eligibility requirements for a UK Student Visa?\", height=120)\n",
    "ask = st.button(\"Get Answer\")\n",
    "\n",
    "if ask and query.strip():\n",
    "    with st.spinner(\"Retrieving top chunks ...\"):\n",
    "        retrieved = retrieve_chunks(query, model, index, chunks, top_k=top_k)\n",
    "    # show retrieved pieces\n",
    "    st.markdown(\"#### Retrieved context (top results)\")\n",
    "    for i, r in enumerate(retrieved, start=1):\n",
    "        src = r.get(\"source\", \"Unknown\")\n",
    "        st.markdown(f\"**{i}. Source:** {src}\")\n",
    "        st.write(r.get(\"text\", \"[no text]\"))\n",
    "\n",
    "    # generate final answer\n",
    "    with st.spinner(\"Generating answer ...\"):\n",
    "        answer, citations = generate_answer(query, model, index, chunks, top_k=top_k, generator=generator, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    st.markdown(\"### Final Answer\")\n",
    "    st.write(answer)\n",
    "\n",
    "    st.markdown(\"### Citations / matched chunks\")\n",
    "    for c in citations:\n",
    "        st.write(\"- \", c[:1000])  # truncate very long chunks in UI\n",
    "\n",
    "    st.download_button(\"Download answer + citations (txt)\", data=answer + \"\\\\n\\\\nCITATIONS:\\\\n\" + \"\\\\n\".join(citations),\n",
    "                       file_name=\"answer_and_citations.txt\")\n",
    "else:\n",
    "    st.info(\"Enter a question and click **Get Answer**. You can upload a chunks .jsonl or provide a local path in the sidebar.\")\n",
    "# write to file\n",
    "with open(\"app_streamlit.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"Wrote app_streamlit.py — next: run it with Streamlit.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
