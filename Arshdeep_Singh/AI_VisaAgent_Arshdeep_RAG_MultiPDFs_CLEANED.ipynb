{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Complete RAG Pipeline for Multiple PDFs\n",
        "\n",
        "This notebook implements a complete Retrieval-Augmented Generation (RAG) system that:\n",
        "- **Processes ALL PDF files** in `/content/data` directory\n",
        "- Creates embeddings for all documents\n",
        "- Builds a unified search index\n",
        "- Enables Q&A across all documents\n",
        "\n",
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_dependencies",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acfa082-7c2c-48d6-fea4-f4fafbcd87d1"
      },
      "source": [
        "# Install required packages for Google Colab\n",
        "!pip install -q PyPDF2\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q transformers\n",
        "!pip install -q torch\n",
        "\n",
        "# Create directories for data storage\n",
        "import os\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "os.makedirs('/content/output', exist_ok=True)\n",
        "\n",
        "print('✅ All dependencies installed successfully!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ All dependencies installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "configuration",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc044472-dc53-44ac-f34d-aa4cf10c98a5"
      },
      "source": [
        "# Configuration and File Paths\n",
        "import os\n",
        "\n",
        "# Simple path configuration for Google Colab\n",
        "DATA_DIR = '/content/data'  # Put all your PDF files here\n",
        "OUTPUT_DIR = '/content/output'\n",
        "\n",
        "# File paths for outputs\n",
        "CHUNKS_FILE = os.path.join(OUTPUT_DIR, 'all_chunks.jsonl')\n",
        "EMBEDDINGS_FILE = os.path.join(OUTPUT_DIR, 'chunks_with_embeddings.jsonl')\n",
        "INDEX_FILE = os.path.join(OUTPUT_DIR, 'faiss_index.idx')\n",
        "\n",
        "# Model configuration\n",
        "EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "GENERATION_MODEL = 'google/flan-t5-base'\n",
        "\n",
        "# Chunking parameters\n",
        "MAX_TOKENS = 256\n",
        "OVERLAP_TOKENS = 32\n",
        "\n",
        "# RAG parameters\n",
        "TOP_K_RESULTS = 5\n",
        "MAX_CONTEXT_LENGTH = 2048\n",
        "\n",
        "print('Configuration loaded successfully!')\n",
        "print(f'Data directory: {DATA_DIR}')\n",
        "print(f'Output directory: {OUTPUT_DIR}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded successfully!\n",
            "Data directory: /content/data\n",
            "Output directory: /content/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_header"
      },
      "source": [
        "## Part 1: Document Processing - All PDFs in Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "helper_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76600fb5-0a69-4155-be87-db35f0da05b6"
      },
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import hashlib\n",
        "from typing import List, Dict, Any\n",
        "import glob\n",
        "\n",
        "def normalize_slug(s: str, repl=\"-\"):\n",
        "    \"\"\"Normalize string to create a clean slug\"\"\"\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", repl, s)\n",
        "    s = re.sub(rf\"{repl}{{2,}}\", repl, s)\n",
        "    s = s.strip(repl)\n",
        "    return s or \"na\"\n",
        "\n",
        "def get_document_hash(content: str) -> str:\n",
        "    \"\"\"Generate SHA256 hash of document content\"\"\"\n",
        "    return hashlib.sha256(content.encode()).hexdigest()\n",
        "\n",
        "def get_all_pdfs(directory: str) -> List[str]:\n",
        "    \"\"\"Get all PDF files in a directory\"\"\"\n",
        "    pdf_pattern = os.path.join(directory, '*.pdf')\n",
        "    pdf_files = glob.glob(pdf_pattern)\n",
        "\n",
        "    # Also check for PDFs with uppercase extension\n",
        "    pdf_pattern_upper = os.path.join(directory, '*.PDF')\n",
        "    pdf_files.extend(glob.glob(pdf_pattern_upper))\n",
        "\n",
        "    return list(set(pdf_files))  # Remove duplicates\n",
        "\n",
        "print('Helper functions loaded!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chunking_functions",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "bffe1479cac54a29b03dceb175c2dd1d",
            "ab69f24939264f95bce95d0ced10e016",
            "7f2e2c0efd8944429f9cdbd97b593a02",
            "59ec021522e4469890f675ea9d329cf2",
            "2257f30b2c2147fd8b8aa54f6a3cf39e",
            "b0ab633271c24b0ea3e07b3509e58b58",
            "965ed19a5fee42118381f9d7f845c0a6",
            "7e66eb37c2e54c71a41c4208fd7bcb30",
            "3e6e5f87f6504a13a245b1c1bb050f6f",
            "7c665069f9584b0e96e03b0142145843",
            "fe7e4cafd3164f48b8f247085e2c1345",
            "81a80ebe540844bdbe1ff5432b98cf08",
            "b552e41be7554872ad7e18b32f7d84d4",
            "7196f102f0a348c8874bf2a914565bb0",
            "a2614ad233184d50be483d897ce86664",
            "f453b72916f74365998284ea24cea3c0",
            "f82c5ce1756f479398cd24a49db496b4",
            "c6f0c20059384615842becbde086e4a4",
            "673b4f11db7042b6bcd073e8e84f8255",
            "4ca7bc2f5f574a188db90a237bb6d94a",
            "46f94961f53247258f70c3c46d28906a",
            "2c7b545635bb48509f2be03d15e43e8e",
            "95876036bc4e4b42b2a9f244663320ea",
            "ae4a874b344a4e3fbfd74bf81665c256",
            "561ccd6efa09478687943e6538cffe8b",
            "2f4218711cbf48c18298c3d06c154842",
            "9e83553336464c33919ebc23817d8ca0",
            "69d9bd8728f4412fb68c6b12a4c0b0d4",
            "51f07543446f4581b803e4887117e262",
            "424da88a542044cd9a99ef9cadef435d",
            "4ef9dc8b6bc146f4bc9f2a3dd2235043",
            "15341547faad4ebc97fe556377c0aef9",
            "3894bb9be1c34e39a967e93413356073",
            "20a601d903f34130bf473dff56b747c4",
            "79eaf54305884217b4d809d6c0196199",
            "418892e3a9b046ce8753567d5c3fe686",
            "e0be8ee58c8c4e939e26cd36298f8421",
            "00f9f6c29de5437ca1d9d6b35f2ba9c5",
            "de4f4f6cb290421ba5970c48d9addf7d",
            "c91a9fd815b348d3853e4229e3a14aa9",
            "3cee36260bbe4c699cbff1105535ca71",
            "138ad5eaaf19423888721ed03ec51157",
            "addaf36e9bd74b1c9a4453855eb36e9a",
            "07ba69bca04a4743a5463b415471c6e7"
          ]
        },
        "outputId": "bb1952bb-6ad8-4953-b836-ca7c6baa933b"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
        "\n",
        "def chunk_by_tokens(text: str, max_tokens=MAX_TOKENS, overlap=OVERLAP_TOKENS):\n",
        "    \"\"\"Split text into overlapping chunks based on token count\"\"\"\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = []\n",
        "\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        end = min(start + max_tokens, len(tokens))\n",
        "        chunk_tokens = tokens[start:end]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "        if end >= len(tokens):\n",
        "            break\n",
        "        start += max_tokens - overlap\n",
        "\n",
        "    return chunks\n",
        "\n",
        "print(f'Tokenizer loaded: {EMBEDDING_MODEL}')\n",
        "print(f'Chunking configured: max_tokens={MAX_TOKENS}, overlap={OVERLAP_TOKENS}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bffe1479cac54a29b03dceb175c2dd1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81a80ebe540844bdbe1ff5432b98cf08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95876036bc4e4b42b2a9f244663320ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20a601d903f34130bf473dff56b747c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded: sentence-transformers/all-MiniLM-L6-v2\n",
            "Chunking configured: max_tokens=256, overlap=32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdf_processing_multiple",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "119c0cb6-f770-4d8a-c3e6-c41fddec7b6c"
      },
      "source": [
        "import PyPDF2\n",
        "import json\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Extract text from a single PDF and return page data\"\"\"\n",
        "    pages_data = []\n",
        "\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            total_pages = len(reader.pages)\n",
        "\n",
        "            for page_num in range(total_pages):\n",
        "                page = reader.pages[page_num]\n",
        "                text = page.extract_text()\n",
        "\n",
        "                if text.strip():\n",
        "                    pages_data.append({\n",
        "                        'page': page_num + 1,\n",
        "                        'text': text\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f'⚠️ Error reading {pdf_path}: {e}')\n",
        "        return []\n",
        "\n",
        "    return pages_data\n",
        "\n",
        "def process_all_pdfs(data_dir: str, output_path: str):\n",
        "    \"\"\"Process ALL PDFs in a directory and create chunks\"\"\"\n",
        "\n",
        "    # Get all PDF files\n",
        "    pdf_files = get_all_pdfs(data_dir)\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f'⚠️ No PDF files found in {data_dir}')\n",
        "        print('Please upload PDF files to /content/data/')\n",
        "        return 0\n",
        "\n",
        "    print(f'Found {len(pdf_files)} PDF files:')\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f'  - {os.path.basename(pdf_file)}')\n",
        "\n",
        "    all_chunks = []\n",
        "    total_chunks = 0\n",
        "\n",
        "    # Process each PDF\n",
        "    for pdf_idx, pdf_path in enumerate(pdf_files, 1):\n",
        "        pdf_name = os.path.basename(pdf_path)\n",
        "        print(f'\\nProcessing [{pdf_idx}/{len(pdf_files)}]: {pdf_name}')\n",
        "\n",
        "        # Extract pages from PDF\n",
        "        pages = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        if not pages:\n",
        "            print(f'  ⚠️ No text extracted from {pdf_name}')\n",
        "            continue\n",
        "\n",
        "        # Get document hash\n",
        "        full_text = ' '.join([p['text'] for p in pages])\n",
        "        doc_hash = get_document_hash(full_text)[:8]\n",
        "\n",
        "        # Create chunks for this PDF\n",
        "        pdf_chunks = 0\n",
        "        for page_data in pages:\n",
        "            page_num = page_data['page']\n",
        "            text = page_data['text']\n",
        "\n",
        "            # Create chunks for this page\n",
        "            chunks = chunk_by_tokens(text)\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_data = {\n",
        "                    'id': f'{doc_hash}_p{page_num}_c{i}',\n",
        "                    'text': chunk,\n",
        "                    'metadata': {\n",
        "                        'page': page_num,\n",
        "                        'chunk_index': i,\n",
        "                        'doc_hash': doc_hash,\n",
        "                        'source': pdf_name,\n",
        "                        'pdf_index': pdf_idx\n",
        "                    }\n",
        "                }\n",
        "                all_chunks.append(chunk_data)\n",
        "                pdf_chunks += 1\n",
        "\n",
        "        print(f'  ✅ Created {pdf_chunks} chunks from {len(pages)} pages')\n",
        "        total_chunks += pdf_chunks\n",
        "\n",
        "    # Save all chunks to file\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for chunk in all_chunks:\n",
        "            f.write(json.dumps(chunk) + '\\n')\n",
        "\n",
        "    print(f'\\n{'='*50}')\n",
        "    print(f'✅ Total: {total_chunks} chunks from {len(pdf_files)} PDFs')\n",
        "    print(f'✅ Saved to: {output_path}')\n",
        "    return total_chunks\n",
        "\n",
        "# Option 1: Upload PDFs manually\n",
        "print('Option 1: Upload PDF files manually')\n",
        "print('Click the button below to upload multiple PDF files:')\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    # Move uploaded files to data directory\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            dest_path = os.path.join(DATA_DIR, filename)\n",
        "            os.rename(filename, dest_path)\n",
        "            print(f'✅ Uploaded: {filename}')\n",
        "        else:\n",
        "            print(f'⚠️ Skipped (not a PDF): {filename}')\n",
        "\n",
        "# Option 2: Process all PDFs already in /content/data/\n",
        "print('\\nOption 2: Process PDFs already in /content/data/')\n",
        "print('Processing all PDF files in the data directory...')\n",
        "\n",
        "# Process ALL PDFs in the directory\n",
        "num_chunks = process_all_pdfs(DATA_DIR, CHUNKS_FILE)\n",
        "\n",
        "if num_chunks == 0:\n",
        "    print('\\n⚠️ No PDFs processed. Creating sample data for testing...')\n",
        "    # Create sample chunks for testing\n",
        "    sample_chunks = [\n",
        "        {'id': 'sample_1', 'text': 'This is a sample document for testing the RAG pipeline.',\n",
        "         'metadata': {'page': 1, 'chunk_index': 0, 'source': 'sample.pdf'}},\n",
        "        {'id': 'sample_2', 'text': 'RAG systems combine retrieval and generation for better answers.',\n",
        "         'metadata': {'page': 1, 'chunk_index': 1, 'source': 'sample.pdf'}},\n",
        "        {'id': 'sample_3', 'text': 'Multiple documents can be processed and searched simultaneously.',\n",
        "         'metadata': {'page': 1, 'chunk_index': 2, 'source': 'sample.pdf'}}\n",
        "    ]\n",
        "    with open(CHUNKS_FILE, 'w') as f:\n",
        "        for chunk in sample_chunks:\n",
        "            f.write(json.dumps(chunk) + '\\n')\n",
        "    print('✅ Created sample chunks for testing')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Option 1: Upload PDF files manually\n",
            "Click the button below to upload multiple PDF files:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-16bb763b-f875-46c8-a31d-db3b0024d677\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-16bb763b-f875-46c8-a31d-db3b0024d677\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Option 2: Process PDFs already in /content/data/\n",
            "Processing all PDF files in the data directory...\n",
            "Found 1 PDF files:\n",
            "  - UK student visa.pdf\n",
            "\n",
            "Processing [1/1]: UK student visa.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2899 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✅ Created 255 chunks from 107 pages\n",
            "\n",
            "==================================================\n",
            "✅ Total: 255 chunks from 1 PDFs\n",
            "✅ Saved to: /content/output/all_chunks.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "check_processed_files",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e0534e-444c-4103-cb0a-eb76b4539d45"
      },
      "source": [
        "# Check what PDFs were processed\n",
        "import json\n",
        "\n",
        "def analyze_chunks(chunks_file: str):\n",
        "    \"\"\"Analyze the chunks file to see what documents were processed\"\"\"\n",
        "    chunks = []\n",
        "    sources = {}\n",
        "\n",
        "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            chunk = json.loads(line)\n",
        "            chunks.append(chunk)\n",
        "            source = chunk['metadata'].get('source', 'unknown')\n",
        "            sources[source] = sources.get(source, 0) + 1\n",
        "\n",
        "    print('📊 Chunk Analysis:')\n",
        "    print(f'Total chunks: {len(chunks)}')\n",
        "    print(f'\\nDocuments processed ({len(sources)}):')\n",
        "    for source, count in sorted(sources.items()):\n",
        "        print(f'  - {source}: {count} chunks')\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Analyze the chunks\n",
        "chunks_data = analyze_chunks(CHUNKS_FILE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Chunk Analysis:\n",
            "Total chunks: 255\n",
            "\n",
            "Documents processed (1):\n",
            "  - UK student visa.pdf: 255 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_header"
      },
      "source": [
        "## Part 2: Embedding Generation for All Documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "embedding_generation",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508,
          "referenced_widgets": [
            "4eb683abb6214a13b6032b9ad1b17091",
            "ebb1b183152c447bba857f80f931db09",
            "0deab10eeceb4fd08e6457dc08b3358a",
            "a4924beada1f4cf89a643d979ebb6306",
            "5eee560a3d67438baf805e110837c921",
            "c889211380bd40b7a0aafd8baf734d29",
            "53bfcf823de8434e8cd0353bd1a13e93",
            "cb5b04dfa927403da39ff5f4c77ad812",
            "f70f093e9e464314a2d17fa881499f02",
            "9c0b7a6a68c04828afeb9efd5a023e81",
            "1000bfc4b31647998fd702976a697843",
            "720abccf447048849cd34c74202da5bd",
            "befb6de9015a40e381550c8ce28e591a",
            "50b026d68a0a416e8b54fe9f795fda74",
            "c56a5c71a8184326a650e95f82ef2e7c",
            "a3e7dd1204114b27811c8be5daa1c902",
            "3dc22dd29fb3499181cbb480aa4c5994",
            "219f87ec924b44b4a30e195f43b1c779",
            "4e6c7e269b7a44538de515021eabd9ed",
            "f3aa5476d71143e29afecd35e1092130",
            "ab91392fe97849399eb2a1383928688c",
            "f65054c0cc114a73af14d48445553b27",
            "d70e5881228d4977be49e257e3b93ad4",
            "d7c18684937741b895bf0a91b32d4c35",
            "f7ee2941259741d0a79fb346c5933987",
            "8562c4e45fa3486ea4ba58dce94873ff",
            "e0860454eac74617b7e1e982e6779048",
            "e28c86c414de452fa486c76569d340f4",
            "108a9be82af04cdb8b9b0bab01adfc32",
            "6fd00bb52b944cc58f143656494b5ce0",
            "d7fe09ca46024993ba8fc5922cb9df89",
            "1482cc026379473dbed0d57e007b6abc",
            "cd6da7bba0e0430a83e93d920f0fa032",
            "aaba5319737245798c77ecada6897fbe",
            "917f75c1f1f0488783c31a3646176c8b",
            "839f1a96a8f548418c620b9f9afb5adb",
            "3d00c4e3a7d94713bcb83189b1c159bc",
            "12a522c3e1a647d3b563166ffb225a57",
            "dab2521d9a744ffca9d6eea5b8f578db",
            "89c4f8bbc51e4b9f84fd899177eee6ea",
            "f92c4f1ce5c74d1694cb857f212b2d35",
            "4c527905841d4c02b70a68fdcfcf3fa2",
            "51f074105f704e4983afc0c52526229b",
            "dd873d8628f44d3a8458f0b047b665ef",
            "6d99d657230a4c669ec000249fad2d08",
            "8aa38ba6b1144a84a6a820cc73674178",
            "9148b066fe2a4e40a4d6d75a193f2270",
            "4f461f9e77bc42ad9cfe427e215ccd5b",
            "da5d18c252b74d51bdf2a5d0226c6c70",
            "f70147c3c9e0421b905a023968854b46",
            "81517620015347e4ad3260f53e854b88",
            "72c741f64e94410fa4d643e488e21988",
            "21e666d7775642cb867c99d44a715465",
            "f785da3b6c85484987af35eda7b38f29",
            "55003a6c450345a4884f9928bb8d7d4a",
            "1a85015adba949968f0ca8ff6e061db8",
            "93f37a2c5a8348318af0f039cc18710f",
            "8c6030dca0cc460a930d7854d9fb7b64",
            "90ed105ddb524c3488290813d79a821c",
            "3e3d0ea1fbba405da39b9f034d0d3eaf",
            "5004cebae0b44684b5139ecc5313b6c3",
            "6b214001e9834ad6bbb6e6d4712385a3",
            "4ae15dbf8bc14d9194cae6a010532328",
            "45a26a5d2caa49cc81a8ae842618c4e3",
            "1592409c068d4c8e985f19d65d5c2cde",
            "124fd6f15c2e431b93bd36b0883e50ab",
            "085c2b4af44846648c36981aa3979218",
            "b817f0da80f14a2fa31043a64f73a807",
            "56a9d22d5784491fad0dc3a06cab64f4",
            "e933912c854e4c3cbbdb1e32bb760173",
            "9e38f047aaf546039a65295782d32b78",
            "bb92b21661c74e679c0f3374f07707e2",
            "94f6753f2bda4f0bb4ab0e5c43c8c99d",
            "3895fecabbe7496b91811592e9a5e022",
            "ab34e5b03e5a43a3a6eaa48063b59805",
            "4d52b98b83b548cb94a1a6848753fd72",
            "1aad938afea6447683beea2065193a94",
            "2f1321bed4574c22ba34539f5aa78c7d",
            "d24baf0e77e74e279b45e9e1d1062359",
            "7413749c185745a88e2b3dde323a35b5",
            "9cc6970f83fa4a9ab58e39c867ad549a",
            "eb159a6e7bbc45aab29a9a2e2c6dd12d",
            "75aeca695e0a40108f4d8956b4f1174e",
            "a743927f1da749858b9c1a0b3afd6aae",
            "dd83d8f0e25343d28851ebe4d36218bb",
            "fab706c8659f4438a4e847efe2bdb240",
            "8903530ce34d4af294296836baf08592",
            "196d2f5a9415405d856b8c55668cb70f"
          ]
        },
        "outputId": "4872fc72-0070-42d8-c686-3863fea323b5"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load embedding model\n",
        "print('Loading embedding model...')\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "print(f'✅ Model loaded: {EMBEDDING_MODEL}')\n",
        "\n",
        "def generate_embeddings(input_file: str, output_file: str):\n",
        "    \"\"\"Generate embeddings for all chunks from all documents\"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # Load chunks\n",
        "    print('Loading chunks...')\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            chunks.append(json.loads(line))\n",
        "\n",
        "    print(f'Loaded {len(chunks)} chunks')\n",
        "\n",
        "    # Count chunks per document\n",
        "    doc_counts = {}\n",
        "    for chunk in chunks:\n",
        "        source = chunk['metadata'].get('source', 'unknown')\n",
        "        doc_counts[source] = doc_counts.get(source, 0) + 1\n",
        "\n",
        "    print(f'From {len(doc_counts)} documents:')\n",
        "    for doc, count in sorted(doc_counts.items()):\n",
        "        print(f'  - {doc}: {count} chunks')\n",
        "\n",
        "    # Generate embeddings in batches\n",
        "    print('\\nGenerating embeddings...')\n",
        "    texts = [chunk['text'] for chunk in chunks]\n",
        "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "    # Save chunks with embeddings\n",
        "    print('Saving embeddings...')\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for chunk, embedding in zip(chunks, embeddings):\n",
        "            chunk['embedding'] = embedding.tolist()\n",
        "            f.write(json.dumps(chunk) + '\\n')\n",
        "\n",
        "    print(f'\\n✅ Generated embeddings for {len(chunks)} chunks')\n",
        "    print(f'✅ Saved to: {output_file}')\n",
        "    print(f'Embedding dimension: {len(embeddings[0])}')\n",
        "    return len(chunks)\n",
        "\n",
        "# Generate embeddings for all documents\n",
        "num_embeddings = generate_embeddings(CHUNKS_FILE, EMBEDDINGS_FILE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4eb683abb6214a13b6032b9ad1b17091"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "720abccf447048849cd34c74202da5bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d70e5881228d4977be49e257e3b93ad4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaba5319737245798c77ecada6897fbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d99d657230a4c669ec000249fad2d08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a85015adba949968f0ca8ff6e061db8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "085c2b4af44846648c36981aa3979218"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded: sentence-transformers/all-MiniLM-L6-v2\n",
            "Loading chunks...\n",
            "Loaded 255 chunks\n",
            "From 1 documents:\n",
            "  - UK student visa.pdf: 255 chunks\n",
            "\n",
            "Generating embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f1321bed4574c22ba34539f5aa78c7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving embeddings...\n",
            "\n",
            "✅ Generated embeddings for 255 chunks\n",
            "✅ Saved to: /content/output/chunks_with_embeddings.jsonl\n",
            "Embedding dimension: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3_header"
      },
      "source": [
        "## Part 3: RAG Implementation - Search Across All Documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faiss_index",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a800df0c-f96f-4add-b8b6-16e08ba614fd"
      },
      "source": [
        "import faiss\n",
        "\n",
        "class MultiDocRAGPipeline:\n",
        "    def __init__(self, embeddings_file: str):\n",
        "        \"\"\"Initialize RAG pipeline for multiple documents\"\"\"\n",
        "        self.chunks = []\n",
        "        self.embeddings = []\n",
        "        self.index = None\n",
        "        self.embedding_model = embedding_model\n",
        "        self.doc_sources = set()\n",
        "\n",
        "        # Load chunks and embeddings\n",
        "        self.load_data(embeddings_file)\n",
        "        # Build FAISS index\n",
        "        self.build_index()\n",
        "\n",
        "    def load_data(self, embeddings_file: str):\n",
        "        \"\"\"Load chunks with embeddings from all documents\"\"\"\n",
        "        print('Loading data from all documents...')\n",
        "        with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                chunk = json.loads(line)\n",
        "                self.chunks.append(chunk)\n",
        "                self.embeddings.append(chunk['embedding'])\n",
        "                self.doc_sources.add(chunk['metadata'].get('source', 'unknown'))\n",
        "\n",
        "        self.embeddings = np.array(self.embeddings, dtype='float32')\n",
        "        print(f'Loaded {len(self.chunks)} chunks from {len(self.doc_sources)} documents')\n",
        "        print(f'Documents: {\", \".join(sorted(self.doc_sources))}')\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"Build FAISS index for similarity search across all documents\"\"\"\n",
        "        print('\\nBuilding unified FAISS index...')\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "\n",
        "        # Create index\n",
        "        dimension = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)  # Inner Product for cosine similarity\n",
        "        self.index.add(self.embeddings)\n",
        "\n",
        "        print(f'✅ Index built with {self.index.ntotal} vectors')\n",
        "        print(f'✅ Ready to search across {len(self.doc_sources)} documents')\n",
        "\n",
        "    def search(self, query: str, k: int = TOP_K_RESULTS, filter_source: str = None):\n",
        "        \"\"\"Search for relevant chunks across all or specific documents\"\"\"\n",
        "        # Encode query\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        query_embedding = np.array(query_embedding, dtype='float32')\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search more if filtering\n",
        "        search_k = k * 3 if filter_source else k\n",
        "        scores, indices = self.index.search(query_embedding, search_k)\n",
        "\n",
        "        # Gather results\n",
        "        results = []\n",
        "        for idx, score in zip(indices[0], scores[0]):\n",
        "            if idx != -1:  # Valid result\n",
        "                chunk = self.chunks[idx]\n",
        "                # Filter by source if specified\n",
        "                if filter_source and chunk['metadata'].get('source') != filter_source:\n",
        "                    continue\n",
        "                results.append({\n",
        "                    'chunk': chunk,\n",
        "                    'score': float(score)\n",
        "                })\n",
        "                if len(results) >= k:\n",
        "                    break\n",
        "\n",
        "        return results\n",
        "\n",
        "    def search_by_document(self, query: str, k_per_doc: int = 2):\n",
        "        \"\"\"Get top results from each document separately\"\"\"\n",
        "        all_results = []\n",
        "\n",
        "        for source in sorted(self.doc_sources):\n",
        "            results = self.search(query, k=k_per_doc, filter_source=source)\n",
        "            if results:\n",
        "                all_results.extend(results)\n",
        "\n",
        "        # Sort by score\n",
        "        all_results.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return all_results\n",
        "\n",
        "    def format_context(self, results, show_source=True):\n",
        "        \"\"\"Format search results as context with source attribution\"\"\"\n",
        "        context_parts = []\n",
        "        seen_sources = set()\n",
        "\n",
        "        for i, result in enumerate(results, 1):\n",
        "            chunk = result['chunk']\n",
        "            text = chunk['text']\n",
        "            metadata = chunk.get('metadata', {})\n",
        "            source = metadata.get('source', 'unknown')\n",
        "            page = metadata.get('page', 'N/A')\n",
        "\n",
        "            seen_sources.add(source)\n",
        "\n",
        "            if show_source:\n",
        "                context_parts.append(f\"[{i}] (Source: {source}, Page: {page})\\n{text}\")\n",
        "            else:\n",
        "                context_parts.append(f\"[{i}] {text}\")\n",
        "\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "        if seen_sources and show_source:\n",
        "            sources_str = \", \".join(sorted(seen_sources))\n",
        "            context = f\"Sources: {sources_str}\\n\\n{context}\"\n",
        "\n",
        "        return context\n",
        "\n",
        "# Initialize multi-document RAG pipeline\n",
        "print('Initializing multi-document RAG pipeline...')\n",
        "rag = MultiDocRAGPipeline(EMBEDDINGS_FILE)\n",
        "print('\\n✅ RAG pipeline ready for multi-document search!')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing multi-document RAG pipeline...\n",
            "Loading data from all documents...\n",
            "Loaded 255 chunks from 1 documents\n",
            "Documents: UK student visa.pdf\n",
            "\n",
            "Building unified FAISS index...\n",
            "✅ Index built with 255 vectors\n",
            "✅ Ready to search across 1 documents\n",
            "\n",
            "✅ RAG pipeline ready for multi-document search!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generation_component",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "3a21fb132c884a0a9edd8873bd80f0a1",
            "ef5a0630e8bd465b800331f884666b3b",
            "625fdc533b7c4dd5a35ab2e2f94df99f",
            "73e12d14694c489c82f00d101446403f",
            "fa60a8dd7d71426f98df8cc2bbd2f92b",
            "db7c38bfc8eb405aa2519f8c5f59a970",
            "db657a99bf8b4f1c9cf851ba7b1b09ed",
            "be61a88676a146498cc5b1eaae2d2a79",
            "97466571419f41bd9a715ef3ff6b9819",
            "09f4e31289a1496d87ef77903d48625e",
            "4e0afe9e70a443f19749a8c471161c11",
            "68664f3f8bc643d9a817737dba7ea3a8",
            "3e1c0d1bc7664799bd9e080b091d972b",
            "044a6e7976fd45ce917461b63bb00559",
            "cebe9e5bd1814769bf01b31ccc5094cf",
            "80bc54ce9a5d43e4bf0d7f3d7cd92edc",
            "54644f77c919458e831898b23428b31e",
            "227dda163bd64332a78ac83a83ce437c",
            "5c035880885d440683335e7852c3f2b9",
            "4dd4fc72dec84f7a8a92d17b6c149f1c",
            "47adaa63928546daab14ed5567ad2260",
            "eaa6f51cdd9546bf818d7919a6b48cd6",
            "5d62fbb913494a379583cc8fa0fadc9e",
            "155eead1630d4023827ac3b473599dcc",
            "e14bd434e85e467b97ff70c4024d47a7",
            "6239416059e94c99be992d9185510cd1",
            "58ca3c4f177e49b38023dd2342474dad",
            "6f68ef5f258544b0a710ec1399dbf1fb",
            "03136c2ba18d410388905bc36d0c088b",
            "3d3e8a13232d49af9ff3e13e61fec57c",
            "ccfc130778ce48cd99097f5703cea6f4",
            "83c8da569d984215bdff7668c128d8d6",
            "5f9f34b9987b40878fb4564c5787112f",
            "0b3d759d5fef4cb187dfa27b9b621191",
            "5b29e9e9386b433d8c1ea8696dc1f31d",
            "8b2fa37e6f7b44148d90963b37e45e9c",
            "6dede447738746a9b226c21f0e8c4cd6",
            "afce36990c3546bd9ac5faf81a9b9adb",
            "6a508d89902047179fa5a417834e2ac0",
            "32dfc9bbb7964b50b1f37202ccb1d9b5",
            "36961ca6a85543808b33797ff4f3dcd4",
            "4d1fc1d035d1480f92e1b70b916c7a22",
            "e1f583e261cd4da0863d46654570c064",
            "f7e8d15ec29d4dda85154fe668e05e45",
            "016457c006bb4cf2be67246dcfa09c88",
            "f4fb4db5f2da4b8280e103e2d762a5a3",
            "60183b04c64941e7810a14674423e4aa",
            "42cf4f081d2649868a9489aa38758bfa",
            "93146b1562de4632bb327a0d2258d752",
            "115e9786825745658088875123577885",
            "b45555d300274b4fa6a368aebeccf370",
            "0a7b682e74ec4d2886663377baac83ef",
            "9044caccfd1440e097bd939c1ad65d04",
            "87e2a89c771f4ed3ae68b6ad6e4ecc89",
            "f3c276ec1861458c9810f4775f84a7bf",
            "59efc9731db94f4cbc03a1ab1fe79257",
            "ecea51a87af84dd7836936b5e2cf5bb5",
            "c08127c744354e84b8332ca0542fdb3d",
            "b3690de8edd944b6aca2fac5ef98e1ec",
            "19e7efb7a36f428a83966c3b24f2fa38",
            "627e557dd6a14be7b1d5abaa4b550f58",
            "6e6d636f47744f57a80926057e063444",
            "f62be598fbb84482a23c06ac008e7025",
            "07b894f50708436391385c04b8544258",
            "a388055684784125b944b5e4d1f94fcf",
            "fe98ee01b4bf4ed0bed56cf3ad629ddf",
            "074c8d7443574a3cbb6bb94514f89941",
            "2b6a1978c3f44076a227e26bb9b8c748",
            "34d6327edf334f1aabeab7aa01722291",
            "b38b7ca491764c398786e64d48c1b64c",
            "ee8b477414a449d6bb19af02037b4cc2",
            "a969f138042044b28479e4ac202b75e5",
            "a8b7a017af3642cd86e7e57c5a862c78",
            "cdead0f64ea8495f8cbe11d1803ce042",
            "21d86193c20d4271b592e4fb9baed013",
            "0e9c78551fc344b58d3e981c27d3355d",
            "1d04365ead244b77ad31e1c813c9174e"
          ]
        },
        "outputId": "53d1340b-e3da-4d30-c46a-daa8081604e6"
      },
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "# Initialize text generation pipeline\n",
        "print(f'Loading generation model: {GENERATION_MODEL}...')\n",
        "try:\n",
        "    generator = pipeline(\n",
        "        'text2text-generation',\n",
        "        model=GENERATION_MODEL,\n",
        "        device=device,\n",
        "        max_length=512\n",
        "    )\n",
        "    print('✅ Generation model loaded')\n",
        "except Exception as e:\n",
        "    print(f'⚠️ Could not load generation model: {e}')\n",
        "    generator = None\n",
        "\n",
        "def generate_answer(query: str, context: str, use_generator: bool = True):\n",
        "    \"\"\"Generate answer based on query and context from multiple documents\"\"\"\n",
        "\n",
        "    # Create prompt\n",
        "    prompt = f\"\"\"Context from multiple documents:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Based on the context provided above from the documents, please answer the question.\n",
        "If the answer cannot be found in the context, say 'I cannot find this information in the provided documents.'\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    if generator and use_generator:\n",
        "        # Use LLM for generation\n",
        "        try:\n",
        "            response = generator(prompt, max_length=200, do_sample=False)[0]['generated_text']\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f'Generation failed: {e}')\n",
        "            return \"Generation failed. Please check the context below.\\n\\n\" + context\n",
        "    else:\n",
        "        # Fallback: return formatted context\n",
        "        return f\"Here is the relevant information found across documents:\\n\\n{context}\"\n",
        "\n",
        "print('✅ Generation component ready!')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: CPU\n",
            "Loading generation model: google/flan-t5-base...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a21fb132c884a0a9edd8873bd80f0a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68664f3f8bc643d9a817737dba7ea3a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d62fbb913494a379583cc8fa0fadc9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b3d759d5fef4cb187dfa27b9b621191"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "016457c006bb4cf2be67246dcfa09c88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59efc9731db94f4cbc03a1ab1fe79257"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "074c8d7443574a3cbb6bb94514f89941"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Generation model loaded\n",
            "✅ Generation component ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa_interface",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328e4cd7-2cfa-4301-b342-60008854d62d"
      },
      "source": [
        "def ask_question(query: str, search_mode: str = 'all', use_llm: bool = True):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: search + generate answer\n",
        "\n",
        "    Args:\n",
        "        query: The question to answer\n",
        "        search_mode: 'all' for unified search, 'per_doc' for per-document search\n",
        "        use_llm: Whether to use LLM for answer generation\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Search mode: {search_mode}\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Search for relevant chunks\n",
        "    print('\\n🔍 Searching across all documents...')\n",
        "\n",
        "    if search_mode == 'per_doc':\n",
        "        # Get top results from each document\n",
        "        results = rag.search_by_document(query, k_per_doc=2)\n",
        "    else:\n",
        "        # Get top results across all documents\n",
        "        results = rag.search(query, k=TOP_K_RESULTS)\n",
        "\n",
        "    if not results:\n",
        "        print('❌ No relevant information found in any document.')\n",
        "        return \"I couldn't find any relevant information for your query in the available documents.\"\n",
        "\n",
        "    # Display search results\n",
        "    print(f'\\n✅ Found {len(results)} relevant chunks:')\n",
        "    sources_found = set()\n",
        "    for i, result in enumerate(results, 1):\n",
        "        source = result['chunk']['metadata'].get('source', 'unknown')\n",
        "        page = result['chunk']['metadata'].get('page', 'N/A')\n",
        "        sources_found.add(source)\n",
        "        print(f\"  {i}. Score: {result['score']:.3f} - {source} (Page {page})\")\n",
        "\n",
        "    print(f'\\n📚 Documents used: {\", \".join(sorted(sources_found))}')\n",
        "\n",
        "    # Format context\n",
        "    context = rag.format_context(results, show_source=True)\n",
        "\n",
        "    # Generate answer\n",
        "    print('\\n💭 Generating answer...')\n",
        "    answer = generate_answer(query, context, use_generator=use_llm)\n",
        "\n",
        "    print('\\n' + '='*60)\n",
        "    print('ANSWER:')\n",
        "    print('='*60)\n",
        "    print(answer)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "print('\\n🚀 Multi-Document RAG System Ready!')\n",
        "print('\\nThe system will search across ALL PDFs in /content/data/')\n",
        "print('\\nExample questions:')\n",
        "print('  - What are the main topics covered in these documents?')\n",
        "print('  - Compare information about [topic] across documents')\n",
        "print('  - What does [specific document] say about [topic]?')\n",
        "print('  - What is the minimum English language requirement for studying a degree-level course in the UK?')\n",
        "print('\\n')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Multi-Document RAG System Ready!\n",
            "\n",
            "The system will search across ALL PDFs in /content/data/\n",
            "\n",
            "Example questions:\n",
            "  - What are the main topics covered in these documents?\n",
            "  - Compare information about [topic] across documents\n",
            "  - What does [specific document] say about [topic]?\n",
            "  - What is the minimum English language requirement for studying a degree-level course in the UK?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "interactive_qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31f4190-1f29-4613-d615-8f5c5a45d1b6"
      },
      "source": [
        "# Interactive Q&A Session - Search Across All Documents\n",
        "# Modify the question below and run the cell\n",
        "\n",
        "question = \"Can you explain the financial requirements for international students applying for a UK Student visa?\"  # <- Change this to your question\n",
        "\n",
        "# Search modes:\n",
        "# - 'all': Get top results across all documents (default)\n",
        "# - 'per_doc': Get top results from each document separately\n",
        "\n",
        "answer = ask_question(question, search_mode='all', use_llm=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Query: Can you explain the financial requirements for international students applying for a UK Student visa?\n",
            "Search mode: all\n",
            "============================================================\n",
            "\n",
            "🔍 Searching across all documents...\n",
            "\n",
            "✅ Found 5 relevant chunks:\n",
            "  1. Score: 0.658 - UK student visa.pdf (Page 40)\n",
            "  2. Score: 0.652 - UK student visa.pdf (Page 39)\n",
            "  3. Score: 0.649 - UK student visa.pdf (Page 39)\n",
            "  4. Score: 0.633 - UK student visa.pdf (Page 102)\n",
            "  5. Score: 0.614 - UK student visa.pdf (Page 43)\n",
            "\n",
            "📚 Documents used: UK student visa.pdf\n",
            "\n",
            "💭 Generating answer...\n",
            "\n",
            "============================================================\n",
            "ANSWER:\n",
            "============================================================\n",
            "international students are required to pay any course fees to the sponsoring institution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "document_specific_search",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a095f04-1fbc-4683-94d5-9a65737086d3"
      },
      "source": [
        "# Search within a specific document\n",
        "\n",
        "def search_in_document(query: str, document_name: str):\n",
        "    \"\"\"Search for information within a specific document\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Searching in: {document_name}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Check if document exists\n",
        "    if document_name not in rag.doc_sources:\n",
        "        print(f\"\\n❌ Document '{document_name}' not found.\")\n",
        "        print(f\"Available documents: {\", \".join(sorted(rag.doc_sources))}\")\n",
        "        return None\n",
        "\n",
        "    # Search in specific document\n",
        "    results = rag.search(query, k=5, filter_source=document_name)\n",
        "\n",
        "    if not results:\n",
        "        print(f\"No relevant information found in {document_name}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n✅ Found {len(results)} relevant chunks in {document_name}:\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        page = result['chunk']['metadata'].get('page', 'N/A')\n",
        "        print(f\"  {i}. Score: {result['score']:.3f} - Page {page}\")\n",
        "\n",
        "    # Format and display results\n",
        "    context = rag.format_context(results, show_source=False)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"RELEVANT INFORMATION:\")\n",
        "    print('='*60)\n",
        "    print(context)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example: Search in a specific PDF\n",
        "# First, let's see what documents are available\n",
        "print(\"Available documents:\")\n",
        "for doc in sorted(rag.doc_sources):\n",
        "    print(f\"  - {doc}\")\n",
        "\n",
        "# Now search in a specific document (change the document name as needed)\n",
        "# search_in_document(\"What is the main topic?\", \"document.pdf\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available documents:\n",
            "  - UK student visa.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compare_documents"
      },
      "source": [
        "# Compare information across multiple documents\n",
        "\n",
        "def compare_across_documents(query: str):\n",
        "    \"\"\"Find and compare information about a topic across all documents\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Comparing across documents: {query}\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Get results from each document\n",
        "    results_by_doc = {}\n",
        "\n",
        "    for source in sorted(rag.doc_sources):\n",
        "        results = rag.search(query, k=2, filter_source=source)\n",
        "        if results:\n",
        "            results_by_doc[source] = results\n",
        "\n",
        "    if not results_by_doc:\n",
        "        print(\"No relevant information found in any document.\")\n",
        "        return\n",
        "\n",
        "    # Display comparison\n",
        "    print(f\"\\n📊 Found relevant information in {len(results_by_doc)} documents:\\n\")\n",
        "\n",
        "    for doc, results in results_by_doc.items():\n",
        "        print(f\"\\n📄 {doc}:\")\n",
        "        print(\"-\" * 40)\n",
        "        for result in results:\n",
        "            chunk = result['chunk']\n",
        "            text = chunk['text'][:200] + \"...\" if len(chunk['text']) > 200 else chunk['text']\n",
        "            page = chunk['metadata'].get('page', 'N/A')\n",
        "            score = result['score']\n",
        "            print(f\"Page {page} (Score: {score:.3f}):\")\n",
        "            print(f\"  {text}\\n\")\n",
        "\n",
        "# Example comparison\n",
        "# compare_across_documents(\"What are the requirements?\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "batch_qa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b8a62e-750e-4926-dbd2-be0b7f9bc83b"
      },
      "source": [
        "# Batch Q&A - Ask multiple questions across all documents\n",
        "\n",
        "questions = [\n",
        "    \"Can Child Students bring dependants to the UK?\",\n",
        "    \"What mandatory information must be included in a valid CAS?\",\n",
        "    \"For how many consecutive days must students show they have held the required funds?\"\n",
        "]\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('BATCH Q&A SESSION - SEARCHING ALL DOCUMENTS')\n",
        "print('='*70)\n",
        "\n",
        "answers = {}\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"\\n\\n[Question {i}/{len(questions)}]\")\n",
        "    answer = ask_question(q, search_mode='all', use_llm=True)\n",
        "    answers[q] = answer\n",
        "    print('\\n' + '-'*50)\n",
        "\n",
        "# Summary\n",
        "print('\\n\\n' + '='*70)\n",
        "print('SUMMARY OF ANSWERS')\n",
        "print('='*70)\n",
        "for q, a in answers.items():\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {a[:200]}...\" if len(a) > 200 else f\"A: {a}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "BATCH Q&A SESSION - SEARCHING ALL DOCUMENTS\n",
            "======================================================================\n",
            "\n",
            "\n",
            "[Question 1/3]\n",
            "\n",
            "============================================================\n",
            "Query: Can Child Students bring dependants to the UK?\n",
            "Search mode: all\n",
            "============================================================\n",
            "\n",
            "🔍 Searching across all documents...\n",
            "\n",
            "✅ Found 5 relevant chunks:\n",
            "  1. Score: 0.656 - UK student visa.pdf (Page 101)\n",
            "  2. Score: 0.654 - UK student visa.pdf (Page 98)\n",
            "  3. Score: 0.652 - UK student visa.pdf (Page 98)\n",
            "  4. Score: 0.644 - UK student visa.pdf (Page 74)\n",
            "  5. Score: 0.629 - UK student visa.pdf (Page 11)\n",
            "\n",
            "📚 Documents used: UK student visa.pdf\n",
            "\n",
            "💭 Generating answer...\n",
            "\n",
            "============================================================\n",
            "ANSWER:\n",
            "============================================================\n",
            "cannot bring dependants with them to the uk\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "[Question 2/3]\n",
            "\n",
            "============================================================\n",
            "Query: What mandatory information must be included in a valid CAS?\n",
            "Search mode: all\n",
            "============================================================\n",
            "\n",
            "🔍 Searching across all documents...\n",
            "\n",
            "✅ Found 5 relevant chunks:\n",
            "  1. Score: 0.645 - UK student visa.pdf (Page 3)\n",
            "  2. Score: 0.609 - UK student visa.pdf (Page 82)\n",
            "  3. Score: 0.602 - UK student visa.pdf (Page 20)\n",
            "  4. Score: 0.597 - UK student visa.pdf (Page 5)\n",
            "  5. Score: 0.597 - UK student visa.pdf (Page 3)\n",
            "\n",
            "📚 Documents used: UK student visa.pdf\n",
            "\n",
            "💭 Generating answer...\n",
            "\n",
            "============================================================\n",
            "ANSWER:\n",
            "============================================================\n",
            "their passport or other travel document proving identity and nationality • biometric residence permit ( if applicable ) • cas reference number student and child student applicants must provide an academic technology approval scheme ( atas ) clearance certificate, if required. student and child student applicants must also provide a valid tuberculosis screening certificate, if required\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "[Question 3/3]\n",
            "\n",
            "============================================================\n",
            "Query: For how many consecutive days must students show they have held the required funds?\n",
            "Search mode: all\n",
            "============================================================\n",
            "\n",
            "🔍 Searching across all documents...\n",
            "\n",
            "✅ Found 5 relevant chunks:\n",
            "  1. Score: 0.653 - UK student visa.pdf (Page 42)\n",
            "  2. Score: 0.645 - UK student visa.pdf (Page 39)\n",
            "  3. Score: 0.606 - UK student visa.pdf (Page 39)\n",
            "  4. Score: 0.550 - UK student visa.pdf (Page 22)\n",
            "  5. Score: 0.550 - UK student visa.pdf (Page 102)\n",
            "\n",
            "📚 Documents used: UK student visa.pdf\n",
            "\n",
            "💭 Generating answer...\n",
            "\n",
            "============================================================\n",
            "ANSWER:\n",
            "============================================================\n",
            "28 day period, unless they are relying on a student loan, an award from a government or international sponsorship agency, or where they are receiving some portion of the funds as other financial sponsor ship from their student sponsor ( as a bursary, for example ). the 28 day period must end no more than 31 days before the application date.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "======================================================================\n",
            "SUMMARY OF ANSWERS\n",
            "======================================================================\n",
            "\n",
            "Q: Can Child Students bring dependants to the UK?\n",
            "A: cannot bring dependants with them to the uk\n",
            "\n",
            "Q: What mandatory information must be included in a valid CAS?\n",
            "A: their passport or other travel document proving identity and nationality • biometric residence permit ( if applicable ) • cas reference number student and child student applicants must provide an acad...\n",
            "\n",
            "Q: For how many consecutive days must students show they have held the required funds?\n",
            "A: 28 day period, unless they are relying on a student loan, an award from a government or international sponsorship agency, or where they are receiving some portion of the funds as other financial spons...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "statistics",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738bcd86-e8cf-4758-e959-f54c3d70db11"
      },
      "source": [
        "# System Statistics and Document Overview\n",
        "\n",
        "def show_statistics():\n",
        "    \"\"\"Display statistics about the indexed documents\"\"\"\n",
        "    print('\\n' + '='*60)\n",
        "    print('📊 SYSTEM STATISTICS')\n",
        "    print('='*60)\n",
        "\n",
        "    # Count chunks per document\n",
        "    doc_stats = {}\n",
        "    total_text_length = 0\n",
        "\n",
        "    for chunk in rag.chunks:\n",
        "        source = chunk['metadata'].get('source', 'unknown')\n",
        "        if source not in doc_stats:\n",
        "            doc_stats[source] = {\n",
        "                'chunks': 0,\n",
        "                'pages': set(),\n",
        "                'text_length': 0\n",
        "            }\n",
        "        doc_stats[source]['chunks'] += 1\n",
        "        doc_stats[source]['pages'].add(chunk['metadata'].get('page', 0))\n",
        "        doc_stats[source]['text_length'] += len(chunk['text'])\n",
        "        total_text_length += len(chunk['text'])\n",
        "\n",
        "    print(f\"\\n📚 Total documents indexed: {len(doc_stats)}\")\n",
        "    print(f\"📄 Total chunks: {len(rag.chunks)}\")\n",
        "    print(f\"📝 Total text: {total_text_length:,} characters\")\n",
        "    print(f\"🔢 Embedding dimension: {rag.embeddings.shape[1]}\")\n",
        "    print(f\"💾 Index size: {rag.index.ntotal} vectors\")\n",
        "\n",
        "    print(\"\\n📖 Document Details:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for doc in sorted(doc_stats.keys()):\n",
        "        stats = doc_stats[doc]\n",
        "        num_pages = len(stats['pages'])\n",
        "        avg_chunk_size = stats['text_length'] // stats['chunks']\n",
        "        print(f\"\\n📄 {doc}\")\n",
        "        print(f\"   Pages: {num_pages}\")\n",
        "        print(f\"   Chunks: {stats['chunks']}\")\n",
        "        print(f\"   Total text: {stats['text_length']:,} chars\")\n",
        "        print(f\"   Avg chunk size: {avg_chunk_size} chars\")\n",
        "\n",
        "show_statistics()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "📊 SYSTEM STATISTICS\n",
            "============================================================\n",
            "\n",
            "📚 Total documents indexed: 1\n",
            "📄 Total chunks: 255\n",
            "📝 Total text: 215,433 characters\n",
            "🔢 Embedding dimension: 384\n",
            "💾 Index size: 255 vectors\n",
            "\n",
            "📖 Document Details:\n",
            "------------------------------------------------------------\n",
            "\n",
            "📄 UK student visa.pdf\n",
            "   Pages: 107\n",
            "   Chunks: 255\n",
            "   Total text: 215,433 chars\n",
            "   Avg chunk size: 844 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save_load",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6e9f61-4a2d-4efb-87da-4a741d5d0581"
      },
      "source": [
        "# Save and Load the Multi-Document Index\n",
        "\n",
        "def save_index(index_path: str = INDEX_FILE):\n",
        "    \"\"\"Save FAISS index and metadata to disk\"\"\"\n",
        "    import pickle\n",
        "\n",
        "    # Save FAISS index\n",
        "    faiss.write_index(rag.index, index_path)\n",
        "    print(f'✅ Index saved to: {index_path}')\n",
        "\n",
        "    # Save metadata\n",
        "    metadata_path = index_path.replace('.idx', '_metadata.pkl')\n",
        "    metadata = {\n",
        "        'num_chunks': len(rag.chunks),\n",
        "        'num_documents': len(rag.doc_sources),\n",
        "        'documents': list(rag.doc_sources),\n",
        "        'embedding_dim': rag.embeddings.shape[1],\n",
        "        'model': EMBEDDING_MODEL\n",
        "    }\n",
        "\n",
        "    with open(metadata_path, 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "    print(f'✅ Metadata saved to: {metadata_path}')\n",
        "\n",
        "    # Display what was saved\n",
        "    print(f\"\\n📊 Saved index contains:\")\n",
        "    print(f\"   - {metadata['num_chunks']} chunks\")\n",
        "    print(f\"   - {metadata['num_documents']} documents\")\n",
        "    print(f\"   - Documents: {\", \".join(metadata['documents'])}\")\n",
        "\n",
        "def load_index(index_path: str = INDEX_FILE):\n",
        "    \"\"\"Load FAISS index from disk\"\"\"\n",
        "    import pickle\n",
        "\n",
        "    if os.path.exists(index_path):\n",
        "        rag.index = faiss.read_index(index_path)\n",
        "        print(f'✅ Index loaded from: {index_path}')\n",
        "        print(f'   Vectors in index: {rag.index.ntotal}')\n",
        "\n",
        "        # Load metadata\n",
        "        metadata_path = index_path.replace('.idx', '_metadata.pkl')\n",
        "        if os.path.exists(metadata_path):\n",
        "            with open(metadata_path, 'rb') as f:\n",
        "                metadata = pickle.load(f)\n",
        "            print(f\"\\n📊 Index contains:\")\n",
        "            print(f\"   - {metadata['num_chunks']} chunks\")\n",
        "            print(f\"   - {metadata['num_documents']} documents\")\n",
        "            print(f\"   - Documents: {\", \".join(metadata['documents'])}\")\n",
        "    else:\n",
        "        print(f'❌ Index file not found: {index_path}')\n",
        "\n",
        "# Save the current index\n",
        "save_index()\n",
        "\n",
        "# Example: Load a previously saved index\n",
        "# load_index()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Index saved to: /content/output/faiss_index.idx\n",
            "✅ Metadata saved to: /content/output/faiss_index_metadata.pkl\n",
            "\n",
            "📊 Saved index contains:\n",
            "   - 255 chunks\n",
            "   - 1 documents\n",
            "   - Documents: UK student visa.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_results",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "e6595acf-60be-4bd9-d393-6231230fdbbc"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "OUTPUT_DIR = \"/content/output\"  # change this to your actual folder\n",
        "\n",
        "def download_rag_files():\n",
        "    \"\"\"Create a zip file with all RAG outputs for download\"\"\"\n",
        "    zip_path = '/content/multi_doc_rag_output.zip'\n",
        "\n",
        "    print('Creating zip file with all outputs...')\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "        for file in os.listdir(OUTPUT_DIR):\n",
        "            file_path = os.path.join(OUTPUT_DIR, file)\n",
        "            if os.path.isfile(file_path):\n",
        "                zipf.write(file_path, file)\n",
        "                print(f'  Added: {file}')\n",
        "\n",
        "    size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "    print(f'\\n✅ Created zip file: {zip_path} ({size_mb:.2f} MB)')\n",
        "\n",
        "    # Start download (only works if <500MB)\n",
        "    files.download(zip_path)\n",
        "    print('✅ Download started!')\n",
        "\n",
        "# Run\n",
        "download_rag_files()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating zip file with all outputs...\n",
            "  Added: faiss_index_metadata.pkl\n",
            "  Added: faiss_index.idx\n",
            "  Added: chunks_with_embeddings.jsonl\n",
            "  Added: all_chunks.jsonl\n",
            "\n",
            "✅ Created zip file: /content/multi_doc_rag_output.zip (2.92 MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f1af89ce-d4bf-4cd6-9018-976133607de9\", \"multi_doc_rag_output.zip\", 3064200)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Download started!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implements a complete multi-document RAG pipeline that:\n",
        "\n",
        "### Key Features\n",
        "- **Processes ALL PDFs** in `/content/data` automatically\n",
        "- **Unified search** across all documents\n",
        "- **Document-specific search** when needed\n",
        "- **Cross-document comparison** capabilities\n",
        "- **Source attribution** for all results\n",
        "\n",
        "### Pipeline Components\n",
        "1. **Batch Document Processing**: Automatically processes all PDFs in the data directory\n",
        "2. **Unified Embedding Generation**: Creates embeddings for all documents\n",
        "3. **Multi-Document Search**: FAISS index for searching across all documents\n",
        "4. **Intelligent Answer Generation**: Context-aware responses with source citations\n",
        "\n",
        "### Usage Instructions\n",
        "\n",
        "1. **Add PDFs to `/content/data/`**:\n",
        "   - Upload multiple PDFs using the upload button\n",
        "   - Or mount Google Drive and copy PDFs to the data folder\n",
        "\n",
        "2. **Run all cells in sequence**:\n",
        "   - The pipeline will automatically process all PDFs\n",
        "   - Creates a unified search index\n",
        "\n",
        "3. **Query your documents**:\n",
        "   - Search across all documents\n",
        "   - Search within specific documents\n",
        "   - Compare information across documents\n",
        "\n",
        "### Tips for Google Colab\n",
        "\n",
        "- **GPU Runtime**: Enable GPU for faster processing (Runtime > Change runtime type > GPU)\n",
        "- **Persistent Storage**: Mount Google Drive to save processed files\n",
        "- **Large Documents**: The system handles multiple large PDFs efficiently\n",
        "- **Batch Upload**: You can upload multiple PDFs at once\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with different search modes\n",
        "- Adjust chunking parameters for your documents\n",
        "- Try different embedding models for domain-specific content\n",
        "- Export the index for use in production systems"
      ]
    }
  ]
}