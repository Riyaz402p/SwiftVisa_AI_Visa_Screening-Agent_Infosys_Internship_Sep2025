# -*- coding: utf-8 -*-
"""RagModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H4TR7mq1Ybh9-Cl1x-ghUymg83dOYTlz
"""

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import json
from transformers import pipeline

!pip install faiss-cpu

# -----------------------------
# 1. Build FAISS Index in rag_pipeline
# -----------------------------
def build_faiss_index(chunks_path):
    # Load model
    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

    # Load chunks from file
    chunks = []
    with open(chunks_path, "r", encoding="utf-8") as f:
        for line in f:
            chunks.append(json.loads(line))

    # Create FAISS index
    # Filter out chunks without 'embedding'
    chunks_with_embeddings = [chunk for chunk in chunks if 'embedding' in chunk]
    if not chunks_with_embeddings:
        raise ValueError("No chunks with 'embedding' found in the file.")

    embedding_dim = len(chunks_with_embeddings[0]['embedding'])
    index = faiss.IndexFlatL2(embedding_dim)
    embeddings = np.array([chunk['embedding'] for chunk in chunks_with_embeddings]).astype('float32')
    index.add(embeddings)

    return model, index, chunks_with_embeddings

# -----------------------------
# 2. Retrieve function
# -----------------------------
def retrieve_chunks(query, model, index, chunks, top_k=5):
    query_embedding = model.encode(query).astype('float32')
    D, I = index.search(np.array([query_embedding]), top_k)
    results = [chunks[i] for i in I[0]]
    return results

# -----------------------------
# 3. Load generator model
# -----------------------------
generator = pipeline(
    "text2text-generation",
    model="google/flan-t5-base",
    tokenizer="google/flan-t5-base",
    device=-1  # CPU; use 0 for GPU
)

# -----------------------------
# 4. Answer generation
# -----------------------------
def generate_answer(query, model, index, chunks, top_k=5):

    # Step 1: Retrieve
    retrieved = retrieve_chunks(query, model, index, chunks, top_k=top_k)

    # Step 2: Prepare context
    context = "\n\n".join([f"Source: {r.get('source', 'Unknown')}\n{r['text']}" for r in retrieved])

    # Step 3: Build prompt
    prompt = f"""
    You are an expert eligibility officer.
    Using only the context below, answer the question truthfully.
    If the answer is not in the context, say "I cannot find relevant information."

    Context:
    {context}

    Question: {query}
    Answer:
    """

    # Step 4: Generate
    output = generator(prompt, max_new_tokens=200)

    # Step 5: Collect citations
    #citations = list({r.get('source', 'Unknown') for r in retrieved})
    citations = [f"{r.get('source', 'Unknown')} â€” {r['text']}" for r in retrieved]

    return output[0]["generated_text"], citations

from google.colab import files
import os

uploaded = files.upload()

chunks_file = list(uploaded.keys())[0]
chunks_path = os.path.join("/content", chunks_file)

model, index, chunks = build_faiss_index(chunks_path)

query = "What are the eligibility requirements for a UK Student Visa?"
answer, citations = generate_answer(query, model, index, chunks, top_k=5)

print("\nFinal Answer:\n", answer)
print("\nCitations:\n", citations)

from google.colab import files
import os

uploaded = files.upload()  # Select 'chunks_with_embeddings_new.jsonl'

chunks_file = list(uploaded.keys())[0]
chunks_path = os.path.join("/content", chunks_file)

model, index, chunks = build_faiss_index(chunks_path)

query = "I am from Canada and applying for a UK Student Visa. Do I need to prove my English language ability?"
answer, citations = generate_answer(query, model, index, chunks, top_k=5)

print("\nFinal Answer:\n", answer)
print("\nCitations:\n", citations)

