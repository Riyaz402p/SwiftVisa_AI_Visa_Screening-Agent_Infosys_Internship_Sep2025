# -*- coding: utf-8 -*-
"""transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UshY3ld7MCqdFhE6bOwQz1xAlFSYtyLk
"""

import torch
import torch.nn as nn
from tokenizers import Tokenizer , models, trainers, pre_tokenizers
from tokenizers.pre_tokenizers import Whitespace
import torch.optim as optim
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

corpus = [
    "artificial intelligence machine learning",
    "deep learning neural networks",
    "data science big data analytics",
    "machine learning algorithms"
]

with open("corpus_1.txt", "w", encoding = "utf-8") as f:
    for line in corpus:
        f.write(line + '\n')

#Train BPE Tokenizer
tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizers = Whitespace()
trainer = trainers.BpeTrainer(vocab_size = 200 , show_progress = True, special_tokens = ["[UNK]", "[CLS]", "[SEP]"])
tokenizer.train(["corpus_1.txt"], trainer)

output  = tokenizer.encode("machine learning algorithms")
print("TOKENS : ", output.tokens)
print("TOKEN_ids : ", output.ids)

# Embeddings
vocab_size = tokenizer.get_vocab_size()
embed_dim = 8
embedding = nn.Embedding(vocab_size, embed_dim)
token_ids = torch.tensor(output.ids)
embeds = embedding(token_ids)

print("Embeddding Shape :", embeds.shape)
print('Sample Embeddibngs:\n', embeds)

# Transfomer
class MiniTransfomer(nn.Module):
    def __init__(self, embed_dim , num_heads = 2):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim , num_heads)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim,embed_dim),
            nn.ReLU(),
            nn.Linear(embed_dim,embed_dim)
        )

    def forward(self, x):
        x = x.unsqueeze(1) # seq length , batch , embed_dim
        attn_out, _ = self.attn(x,x,x)
        x= attn_out + x # residual
        x = self.ff(x)+x
        return x.squeeze(1)

model = MiniTransfomer(embed_dim)

#Training loop
optimizer = optim.Adam(list(model.parameters())  + list(embedding.parameters()), lr= 0.01  )
loss_fn = nn.CrossEntropyLoss()

for epoch in range(20):
    optimizer.zero_grad()

    # recomute embeddings each epoch
    embeds = embedding(token_ids)
    inputs = embeds[:-1]
    targets = token_ids[1:]

    out = model(inputs)
    logits = out @ embedding.weight.T
    loss = loss_fn(logits,targets )

    loss.backward()
    optimizer.step()

    print(f"\n Epoch {epoch}")
    print("Transformer Output shape: ", out.shape)
    print("logits  shape: ", logits.shape)
    print("loss:", loss.item())

# Visulaize

final_embeds = embedding.weight.detach().numpy()

pca = PCA(n_components=2).fit_transform(final_embeds)

plt.figure(figsize= (7,7))

for word, idx in tokenizer.get_vocab().items():
    plt.scatter(pca[idx, 0], pca[idx,1])
    plt.text(pca[idx,0]+0.01, pca[idx,1]+0.01, word)
    plt.title("Word Embeddinfs - PCA")
plt.show()